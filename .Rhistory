words <- tolower(words)
words <- removeWords(words, stopwords("english"))
words <- removeWords(words, c("shit","piss","fuck","cunt","cocksucker","motherfucker","tits"))
words <- removePunctuation(words)
words <- wordStem(words)
wordStem("king")
wordStem("let us king")
"king" %in% training$name
word <- "dancing"
words <- concatenate(input,word)
words <- clean_words(words)
words <- concatenate(input,word)
words <- removeNumbers(words)
words <- tolower(words)
words <- removeWords(words, stopwords("english"))
words <- removeWords(words, c("shit","piss","fuck","cunt","cocksucker","motherfucker","tits"))
words <- removePunctuation(words)
words <- wordStem(words)
words <- concatenate(strsplit(input," ")[[1]])
words <- concatenate(input,word)
concatenate(strsplit(input," ")
)
concatenate(strsplit(input," ")[[1]])
clean_words <- function(words){
words <- removeNumbers(words)
words <- tolower(words)
words <- removeWords(words, stopwords("english"))
words <- removeWords(words, c("shit","piss","fuck","cunt","cocksucker","motherfucker","tits"))
words <- removePunctuation(words)
words <- wordStem(words)
words <- concatenate(strsplit(words," ")[[1]])
words
}
words <- concatenate(input,word)
words <- clean_words(words)
prob_ngrams("let us","king")
count_word_freq(words)
concatenate(strsplit(input," ")[[1]][-1])
concatenate(strsplit(input," ")[[1]][-1])
prob_ngrams("us",word)
count_word_freq("us danc")
count_word_freq <- function(words){
words <- clean_words(words)
n <- wordcount(words)
if(n == 0) return(0)
if(n > 4) stop("Model has too many parameters")
if(n == 1){
index <- grep(paste0("^",words,"$"),training$name)
if(length(index) == 0) return(0)
else{
return(training$freq[index])
}
}
else{
temp <- paste0("^",words," ")
index <- grep(temp,worddict[[n - 1]]$ngrams)
if(length(index) == 0) return(0)
else{
return(worddict[[n - 1]]$freq[index])
}
}
}
count_word_freq("us danc")
count_word_freq("us danc")/count_word_freq("us")
count_word_freq("us")
count_word_freq("just")
us %in% training$name
"us" %in% training$name
stopwords("english")
"us" %in% stopwords("english")
prob_ngrams("Chao","Liu")
prob_ngrams("ass","ass")
prob_ngrams("see","you")
prob_ngrams("see","me")
prob_ngrams("see","them")
prob_ngrams("see","something")
concatenate(strsplit(input," ")[[1]][-1])
concatenate(strsplit("us"," ")[[1]][-1])
prob_ngrams <- function(input, word){
words <- concatenate(input,word)
words <- clean_words(words)
if(count_word_freq(word) == 0) return(0)
if(count_word_freq(words) > 0){
return(count_word_freq(words)/count_word_freq(input))
}
else{
input <- concatenate(strsplit(input," ")[[1]][-1])
return(0.4*prob_ngrams(input,word))
}
}
prob_ngrams("see","something")
prob_ngrams <- function(input, word){
words <- concatenate(input,word)
words <- clean_words(words)
if(count_word_freq(clean_words(word)) == 0) return(0)
if(count_word_freq(words) > 0){
return(count_word_freq(words)/count_word_freq(input))
}
else{
input <- concatenate(strsplit(input," ")[[1]][-1])
return(0.4*prob_ngrams(input,word))
}
}
prob_ngrams("see","sth")
prob_ngrams("see","something")
count_word_freq(clean_words("something"))
word <- "something"
input <- "see"
words <- concatenate(input,word)
words <- clean_words(words)
count_word_freq(words)
count_word_freq(words)
input <- concatenate(strsplit(input," ")[[1]][-1])
words <- concatenate(input,word)
words <- clean_words(words)
count_word_freq(words)
input <- concatenate(strsplit(input," ")[[1]][-1])
clean_words(word)
clean_words(word) %in% training$name
training$name[grep(clean_words(word),training$name)]
training$name[grep(clean_words("something"),training$name)]
training$name[grep("something",training$name)]
count_word_freq("something")
training[grep(clean_words(word),training$name),]
clean_words("somethin")
prob <- function(input){
input <- clean_words(input)
x <- training$name
sapply(x, function(x){prob_ngrams(input,x)})
}
prob("us")
prob_ngrams("let","see")
x <- proc.time()
prob_ngrams("let","see")
proc.time() - x
prob_ngrams("just","now")
count_word_freq(clean_words("now")
)
count_word_freq("just now")
count_word_freq("just now")/count_word_freq("just")
prob_ngrams <- function(input, word){
words <- concatenate(input,word)
words <- clean_words(words)
if(count_word_freq(clean_words(word)) == 0) return(0)
l <- count_word_freq(words)
if(l > 0){
return(l/count_word_freq(input))
}
else{
input <- concatenate(strsplit(input," ")[[1]][-1])
return(0.4*prob_ngrams(input,word))
}
}
prob_ngrams("just","now")
x <- proc.time()
prob_ngrams("just","now")
proc.time() - x
x <- proc.time()
prob_ngrams("just","see")
proc.time() - x
x <- proc.time()
prob_ngrams("let","see")
proc.time() - x
wujie
x <- proc.time()
pred_model("Chao Liu rock dat")
proc.time() - x
prob <- function(input){
input <- clean_words(input)
x <- training$name
sapply(x, function(word){prob_ngrams(input,word)})
}
prob_ngrams("","just")
length("")
wordcount("")
wordcount(" just")
x <- proc.time()
sum(training$freq)
proc.time() - x
prob_ngrams <- function(input, word){
words <- concatenate(input,word)
words <- clean_words(words)
word_f <- count_word_freq(clean_words(word))
if(word_f == 0) return(0)
if(wordcount(input) == 0) return(word_f / sum(training$freq))
l <- count_word_freq(words)
if(l > 0){
return(l/count_word_freq(input))
}
else{
input <- concatenate(strsplit(input," ")[[1]][-1])
return(0.4*prob_ngrams(input,word))
}
}
prob_ngrams("","just")
prob_ngrams("","us")
prob_ngrams("see","us")
prob_ngrams("see","me")
head(prob("just"),20)
system.time(sum(training$freq))
x <- proc.time()
pointer <- 0
prob <- numeric()
l <- length(training$freq)
for(pointer in 1:l){
prob[pointer] <- prob_ngrams("just", training$name[pointer])
}
proc.time() - x
head(prob)
head(prob,100)
306.64 * 7923/1034
306.64 * 7923/1034 / 3600
which.min(prob)
training$name[which.min(prob)]
training$name[which.max(prob)]
prob_ngrams("just","one")
prob_ngrams("just","offic")
which.max(prob)
count_word_freq("just offic")
count_word_freq(clean_words("offic"))
count_word_freq("just offic")
sum(training$freq)
sum(prob == 1)
source("Functionality.R")
prob_ngrams("just","offic")
input <- "just"
word <- "word"
word <- "offic"
word_f <- count_word_freq(clean_words(word))
wordcount(input)
words <- concatenate(input,word)
words <- clean_words(words)
words <- clean_words(words)
words <- clean_words(words)
words <- clean_words(words)
words <- clean_words(words)
words <- clean_words(words)
words <- clean_words(words)
words <- clean_words(words)
words <- clean_words(words)
words <- clean_words(words)
words <- clean_words(words)
words <- clean_words(words)
words <- clean_words(words)
words <- clean_words(words)
clean_words("office")
clean_words("offic")
clean_words("offic")
clean_words("offic")
words <- concatenate(input,word)
words <- removeNumbers(words)
words <- tolower(words)
words <- removeWords(words, stopwords("english"))
words <- removeWords(words, c("shit","piss","fuck","cunt","cocksucker","motherfucker","tits"))
words <- removePunctuation(words)
words <- wordStem(words)
words <- concatenate(strsplit(words," ")[[1]])
words
prob_ngrams <- function(input, word){
word_f <- count_word_freq(clean_words(word))
if(word_f == 0) return(0)
if(wordcount(input) == 0) return(word_f / sum(training$freq))
input <- clean_words(input)
words <- concatenate(input,word)
l <- count_word_freq(words)
if(l > 0){
return(l/count_word_freq(input))
}
else{
input <- concatenate(strsplit(input," ")[[1]][-1])
return(0.4*prob_ngrams(input,word))
}
}
prob_ngrams("just","offic")
prob_ngrams("just","one")
x <- proc.time()
pointer <- 0
prob <- numeric()
l <- length(training$freq)
for(pointer in 1:l){
prob[pointer] <- prob_ngrams("just", training$name[pointer])
}
training$name[which.max(prob)]
training$name[which.min(prob)]
sum(prob == 1)
sum(prob == 0)
count_word_freq <- function(words){
n <- wordcount(words)
if(n == 0) return(0)
if(n > 4) stop("Model has too many parameters")
if(n == 1){
index <- grep(paste0("^",words,"$"),training$name)
if(length(index) == 0) return(0)
else{
return(training$freq[index])
}
}
else{
temp <- paste0("^",words," ")
index <- grep(temp,worddict[[n - 1]]$ngrams)
if(length(index) == 0) return(0)
else{
return(worddict[[n - 1]]$freq[index])
}
}
}
prob_ngrams <- function(input, word){
word_f <- count_word_freq(word)
if(word_f == 0) return(0)
if(wordcount(input) == 0) return(word_f / sum(training$freq))
input <- clean_words(input)
words <- concatenate(input,word)
l <- count_word_freq(words)
if(l > 0){
return(l/count_word_freq(input))
}
else{
input <- concatenate(strsplit(input," ")[[1]][-1])
return(0.4*prob_ngrams(input,word))
}
}
prob <- function(input){
input <- clean_words(input)
x <- training$name
sapply(x, function(word){prob_ngrams(input,word)})
}
source("Functionality.R")
x <- proc.time()
pointer <- 0
prob <- numeric()
l <- length(training$freq)
for(pointer in 1:l){
prob[pointer] <- prob_ngrams("just", training$name[pointer])
}
proc.time() - x
7923/2437.27
1/(7923/2437.27)
training$name[which.max(prob)]
training$name[head(sorted(prob,decresing = TRUE),20)]
training$name[head(sort(prob,decresing = TRUE),20)]
training$name[head(sort(prob,decreasing = TRUE),20)]
head(sort(prob,decreasing = TRUE),20)
which(prob = head(sort(prob,decreasing = TRUE),20))
which(prob == head(sort(prob,decreasing = TRUE),20))
?which
training$name[which(prob == sort(prob,decreasing = TRUE)[2])]
training$name[which(prob == sort(prob,decreasing = TRUE)[3])]
training$name[which(prob == sort(prob,decreasing = TRUE)[4])]
training$name[which(prob == sort(prob,decreasing = TRUE)[5])]
training$name[which(prob == sort(prob,decreasing = TRUE)[6])]
training$name[which(prob == sort(prob,decreasing = TRUE)[7])]
training$name[which(prob == sort(prob,decreasing = TRUE)[8])]
training$name[which(prob == sort(prob,decreasing = TRUE)[9])]
training$name[which(prob == sort(prob,decreasing = TRUE)[10])]
hist(prob, freq = FALSE, col = "lightgrey")
mean(prob == 0)
mean(prob < 0.01)
?which.max
pred_model("just")
pred_model("happy")
pred_ngram("happy")
pred_ngram("")
simple_backoff_model <- function(input){
while(pred_ngram(input)[1] == " ") {
input <- concatenate(strsplit(input," ")[[1]][-1])
if(wordcount(input) == 1 | wordcount(input) == 0 ) {
return(" ")
}
}
pred_ngram(input)
}
pred_model("happy")
x <- proc.time()
pointer <- 0
prob <- numeric()
l <- length(training$freq)
for(pointer in 1:l){
prob[pointer] <- prob_ngrams("happy", training$name[pointer])
}
proc.time() - x
323.66/1235
training$name[which.max(prob)]
training$name[which.min(prob)]
max(prob)
??"ngram"
x <- proc.time()
pointer <- 0
prob <- numeric()
l <- length(training$freq)
for(pointer in 1:l){
prob[pointer] <- prob_ngrams("Chao", training$name[pointer])
}
proc.time() - x
1243.53/4834
training$name
training$name[which.max(prob)]
mean(prob==0)
mean(prob==1)
max(prob)
head(training,20)
head(training)
pred_model <- function(input){
l <- wordcount(input)
if(l >= 4){
words <- concatenate(strsplit(input," ")[[1]][(l-2):l])
}
else {
words <- input
}
res <- simple_backoff_model(words)
if(res[1] == " ") res <- training$name[1:5]
else return(res)
}
pred_model("chao")
pred_model("chaotic")
pred_model <- function(input){
l <- wordcount(input)
if(l >= 4){
words <- concatenate(strsplit(input," ")[[1]][(l-2):l])
}
else {
words <- input
}
res <- simple_backoff_model(words)
if(res[1] == " ") res <- training$name[1:5]
res
}
pred_model("chaotic")
pred_model("chao")
pred_model("lalalal just")
pred_model("just")
pred_model("lala just")
pred_model("do you like")
pred_model("like")
simple_backoff_model("like")
head(wordtable3)
head(wordtable2)
head(wordtable)
pred_ngram("like")
simple_backoff_model <- function(input){
while(pred_ngram(input)[1] == " ") {
input <- concatenate(strsplit(input," ")[[1]][-1])
if(wordcount(input) == 0 ) {
return(" ")
}
}
pred_ngram(input)
}
pred_model("leu us")
pred_model("let us")
pred_model("lalalalal la ass hahahah let")
pred_model("do you like")
pred_model("What the hell")
pred_model("I cannot image")
pred_model("This is awesome")
pred_model("This is")
pred_model("just like")
pred_model("this is")
head(worddict[[2]])
grep(paste0("^", "this is", " .+"), worddict[[n - 1]]$ngrams)
grep(paste0("^", "this is", " .+"), worddict[[2]]$ngrams)
clean_words("this is")
pred_ngram <- function(words){
words <- clean_words(words)
if(wordcount(words) == 0) return(" ")
n <- wordcount(words) + 1
if(n > 4) stop("Model has too many parameters")
res <- character()
index <- grep(paste0("^",words," .+"), worddict[[n-1]]$ngrams)
if(length(index) == 0){
return(" ")
}
m <- min(length(index),5)
temp <- worddict[[n-1]][index,]
#removeWords(temp$ngrams[1:n],paste(word, ""))
for(i in 1:m){
res[i] <- strsplit(temp$ngrams[i]," ")[[1]][length(strsplit(temp$ngrams[i]," ")[[1]])]
}
res
}
pred_model("This is")
shiny::runApp('F:/Data Science/Homework/Developing Data Products')
library(shiny)
?wordcloud
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
?wordcloud
sys.date()
system.date()
system.time()
Sys.Date()
x <- proc.time()
pred_model("Data Science Rocks")
proc.time() - x
pred_ngram("seeing")
prob_ngrams("just","see")
shiny::runApp()
## Overview
shiny::runApp()
shiny::runApp()
rsconnect::setAccountInfo(name='chaoliu',
token='C31A014037A168ECFD608FA7F984D3B3',
secret='<SECRET>')
rsconnect::setAccountInfo(name='chaoliu', token='C31A014037A168ECFD608FA7F984D3B3', secret='5JQ8jkzzb6KZvwly64etWp5EMEr8oNPc2V+zPCYk')
shiny::runApp('F:/Data Science/Capstone/upload')
rsconnect::setAccountInfo(name='chaoliu', token='C31A014037A168ECFD608FA7F984D3B3', secret='5JQ8jkzzb6KZvwly64etWp5EMEr8oNPc2V+zPCYk')
library(rsconnect)
shiny::runApp('F:/Data Science/Capstone/upload')
shiny::runApp('F:/Data Science/Capstone/upload')
shiny::runApp('F:/Data Science/Capstone/upload')
